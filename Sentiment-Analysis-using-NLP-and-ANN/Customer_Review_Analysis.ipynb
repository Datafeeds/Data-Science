{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pro = open('ProsCons/IntegratedPros.txt')\n",
    "pro_data = pro.readlines()\n",
    "pro.close()\n",
    "\n",
    "con = open('ProsCons/IntegratedCons.txt')\n",
    "con_data = con.readlines()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pro_data = [w.replace('<Pros>', '') for w in pro_data]\n",
    "pro_data = [w.replace('</Pros>\\n', '') for w in pro_data]\n",
    "\n",
    "con_data = [w.replace('<Cons>', '') for w in con_data]\n",
    "con_data = [w.replace('</Cons>\\n', '') for w in con_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_pro = pd.DataFrame({'Review':pro_data, 'Label':1})\n",
    "df_con = pd.DataFrame({'Review':con_data, 'Label':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([df_pro, df_con], ignore_index  = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Easy to use, economical!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Digital is where it's at...down with developin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Good image quality, 3x optical zoom, macro mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Awesome features/easy to use/fun/versatile/low...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>intuitive, user friendly</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                             Review\n",
       "0      1                           Easy to use, economical!\n",
       "1      1  Digital is where it's at...down with developin...\n",
       "2      1  Good image quality, 3x optical zoom, macro mod...\n",
       "3      1  Awesome features/easy to use/fun/versatile/low...\n",
       "4      1                           intuitive, user friendly"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cleaning the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ishwor.Bhatta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# punctuations, steming, upper/lower case, irrelevant words(the, on etc), tokenization\n",
    "\n",
    "import re \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(dataset)):\n",
    "    # text cleaning\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) # removing punctuations, numbers(only keeping\n",
    "                                                              # letter from a-z or A-Z)\n",
    "    review = review.lower() # converting to lowercase\n",
    "\n",
    "    # removing the irrelevant texts\n",
    "    review = review.split()\n",
    "    # review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "\n",
    "    # steming\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Creating a Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# creating a sparse matrix with unique words from corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 2000)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fitting Random Forest Classification to the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=50, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rcf_classifier = RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state = 0)\n",
    "rcf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_rcf = rcf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_rfc = confusion_matrix(y_test, y_pred_rcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Performance Measure of Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance -----------------\n",
      "Accuracy: 89.63%\n",
      "Precision: 88.94%\n"
     ]
    }
   ],
   "source": [
    "accuracy_rcf = (cm_rfc[0][0] + cm_rfc[1][1])/ sum(sum(cm_rfc))\n",
    "precision_rcf = cm_rfc[1][1]/(cm_rfc[1][1] + cm_rfc[0][1])\n",
    "print('Random Forest Performance -----------------')\n",
    "print('Accuracy: {0:.2f}%'.format(accuracy_rcf * 100))\n",
    "print('Precision: {0:.2f}%'.format(precision_rcf * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using Artificial Neural Net Classification Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Setting Up ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initializing the ANN\n",
    "ann_classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "# we will have 2000 input nodes(for 2000 independent variables)\n",
    "# The best activation function could be rectifier function for the hidden layer and sigmoid function for the output layer\n",
    "# We will also be able to find the rank of probability that customer could leave the bank\n",
    "ann_classifier.add(Dense(output_dim = 1000,# number of nodes in the hidden layer being added. Usual practice is to take an average of number of layers in i/p layer and o/p layers. Or performance tuning by K-fold cross validation \n",
    "init = 'uniform' , # initialize the weights to small number close to 0\n",
    "activation = 'relu', # rectifier activation function for hidden layer\n",
    "input_dim = 2000 # number of nodes in th i/p layer( # of independent variable)\n",
    ")) # all the NN parameters are defined here\n",
    "\n",
    "# Adding additional hidden layer\n",
    "ann_classifier.add(Dense(output_dim = 1000, init = 'uniform', activation = 'relu')) # input_dim is required only for first hidden layer as the NN model does not know how many nodes are at the input but after first hidden layet, the model knows how many input are there in the following hidden layers\n",
    "\n",
    "# Adding additional hidden layer\n",
    "ann_classifier.add(Dense(output_dim = 500, init = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "ann_classifier.add(Dense(output_dim = 1, # since we are expecting binary output, we need 1 output node. \n",
    "init = 'uniform', \n",
    "activation = 'sigmoid' # sigmoid activation function for output layer\n",
    "# for muti-value output categories, we need output_dim = nimber of categories and activation = 'soft_max'\n",
    "# soft_max is similar to sigmoid function but applied to dependent varibale that has more than 2 categories\n",
    ")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying stochastic gradient descent in the entire NN\n",
    "ann_classifier.compile(\n",
    "optimizer = 'adam', # algorithm to find optimal set of weights for NN\n",
    "loss = 'binary_crossentropy', # loss function within the stochastic gradient descent algorithm (i.e. in 'adam' algorithm\n",
    "# binary_crossentropy -> for binary o/p and categorical_crossentropy -> for categorical o/p\n",
    "metrics = ['accuracy'] # accuracy criterion to evaluate the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fitting the ANN to the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x186f35c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_classifier.fit(X_train, y_train, \n",
    "batch_size = 10, # whether to update the weights after each observation or after a batch of observations(backpropagation)\n",
    "nb_epoch = 50 # defines number of iterations\n",
    "# for both batch_size and nb_epoch, no optimal value by default. Need to find the best value by experimentation or performance tuning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_ann = ann_classifier.predict(X_test)\n",
    "\n",
    "# since the y_pred here is the probabilities instead of binary value, we need to convert these probabilities into a binary value. For this weed to set a threshold to distinguish between 1 and 0.\n",
    "# for sensitive information we need higher threshold\n",
    "# let's choose 50% as the threshold here\n",
    "y_pred_ann = (y_pred_ann > 0.5) # this gives true/false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_ann = confusion_matrix(y_test, y_pred_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.6 Performance Measure of ANN Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning Performance -----------------\n",
      "Accuracy: 88.56%\n",
      "Precision: 84.81%\n"
     ]
    }
   ],
   "source": [
    "accuracy_ann = (cm_ann[0][0] + cm_ann[1][1])/ sum(sum(cm_ann))\n",
    "precision_ann = cm_ann[1][1]/(cm_ann[1][1] + cm_ann[0][1])\n",
    "print('Deep Learning Performance -----------------')\n",
    "print('Accuracy: {0:.2f}%'.format(accuracy_ann * 100))\n",
    "print('Precision: {0:.2f}%'.format(precision_ann * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deep learning approach did not perform better than Random Forest model with random selection of hyperparameters. We could probably achieve better performance with a bit of hyperparameter tuning. However, it becomes highly coputation intensive to perform tuning with different parameters in the algorithm we used for ANN here. Also, the RF classifier already provided a satisfied performance. Hence, the Random Forest classifier approcah could be implemented in production. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
